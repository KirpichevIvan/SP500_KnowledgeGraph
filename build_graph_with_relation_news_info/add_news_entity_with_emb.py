import os
import json
import time
from datetime import datetime
import pandas as pd
from tqdm import tqdm
from dotenv import load_dotenv, find_dotenv
from neo4j import GraphDatabase
from openai import OpenAI
import ollama

load_dotenv(find_dotenv())


class NewsGraphPipeline:
    def __init__(self):
        self.neo4j_uri = os.getenv("NEO4J_URI")
        self.neo4j_auth = (os.getenv("NEO4J_USERNAME"), os.getenv("NEO4J_PASSWORD"))
        self.polza_key = os.getenv("POLZA_API_KEY")

        self.driver = GraphDatabase.driver(self.neo4j_uri, auth=self.neo4j_auth)
        self.client = OpenAI(api_key=self.polza_key, base_url="https://api.polza.ai/api/v1")

        self.embedding_model = "nomic-embed-text"
        self.vector_dim = 768
        print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é Ollama: {self.embedding_model}")

    def close(self):
        self.driver.close()

    def _get_vector(self, text):
        """–ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –≤ —Å–ø–∏—Å–æ–∫ float —á–µ—Ä–µ–∑ Ollama"""
        if not text: return None
        try:
            response = ollama.embeddings(model=self.embedding_model, prompt=text)
            return response['embedding']
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ Ollama: {e}")
            return None

    def setup_knowledge_base_vectors(self):
        """
        –ü–†–û–¶–ï–î–£–†–ê –ü–û–î–ì–û–¢–û–í–ö–ò:
        1. –ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ —É–∑–ª—ã (Company, Fund, Person, Location).
        2. –í–µ—à–∞–µ—Ç –Ω–∞ –Ω–∏—Ö –º–µ—Ç–∫—É :Searchable.
        3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–µ–∫—Ç–æ—Ä –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ —É–∑–µ–ª.
        4. –°–æ–∑–¥–∞–µ—Ç –µ–¥–∏–Ω—ã–π –∏–Ω–¥–µ–∫—Å.
        """
        print("\nüèóÔ∏è === –ü–û–î–ì–û–¢–û–í–ö–ê –í–ï–ö–¢–û–†–ù–û–ô –ë–ê–ó–´ ===")

        target_labels = ["Company", "Person", "Fund", "City", "Country"]

        query_fetch = f"""
        MATCH (n)
        WHERE any(label in labels(n) WHERE label IN $targets)
        RETURN elementId(n) as id, labels(n) as labels, n.name as name, 
               n.ticker as ticker, n.description as desc
        """

        with self.driver.session() as session:
            print("1. –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∫—É :Searchable –≤—Å–µ–º —Å—É—â–Ω–æ—Å—Ç—è–º...")
            for label in target_labels:
                session.run(f"MATCH (n:{label}) SET n:Searchable")

            result = session.run(query_fetch, targets=target_labels)
            nodes = list(result)
            print(f"2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è {len(nodes)} —É–∑–ª–æ–≤ (—á–µ—Ä–µ–∑ Ollama)...")

            for record in tqdm(nodes, desc="Vectorizing"):
                main_label = [l for l in record['labels'] if l != 'Searchable'][0]
                text = f"{main_label}: {record['name']}"

                if record['ticker']:
                    text += f" ({record['ticker']})"
                if record['desc']:
                    text += f". Info: {str(record['desc'])[:200]}"

                vector = self._get_vector(text)

                if vector:
                    session.run("""
                        MATCH (n) WHERE elementId(n) = $id
                        CALL db.create.setNodeVectorProperty(n, 'embedding', $vec)
                    """, id=record['id'], vec=vector)

            print("3. –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ 'unified_entity_index'...")
            try:
                session.run("DROP INDEX unified_entity_index IF EXISTS")

                session.run(f"""
                    CREATE VECTOR INDEX unified_entity_index IF NOT EXISTS
                    FOR (n:Searchable) ON (n.embedding)
                    OPTIONS {{indexConfig: {{
                        `vector.dimensions`: {self.vector_dim},
                        `vector.similarity_function`: 'cosine'
                    }}}}
                """)
                time.sleep(5)
            except Exception as e:
                print(f"‚ö†Ô∏è Index creation info: {e}")

        print("‚úÖ –ë–∞–∑–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–º—É –ø–æ–∏—Å–∫—É.")

    def resolve_entity(self, name_query, threshold=0.60):
        """
        –ò—â–µ—Ç –±–ª–∏–∂–∞–π—à–∏–π —É–∑–µ–ª –≤ –±–∞–∑–µ –ø–æ —Å–º—ã—Å–ª—É.
        """
        if not name_query or len(name_query) < 2: return None

        vector = self._get_vector(name_query)
        if not vector: return None

        query = """
        CALL db.index.vector.queryNodes('unified_entity_index', 1, $vec)
        YIELD node, score
        WHERE score >= $thresh
        RETURN node, labels(node) as lbls, score
        """

        with self.driver.session() as session:
            result = session.run(query, vec=vector, thresh=threshold).single()

            if result:
                node = result['node']
                labels = result['lbls']
                real_type = [l for l in labels if l != 'Searchable'][0]

                key_field = "ticker" if real_type == 'Company' else "name"
                entity_id = node.get(key_field)

                return {
                    "id": entity_id,
                    "type": real_type,
                    "key_field": key_field,
                    "name": node.get('name'),
                    "score": result['score']
                }
        return None

    def analyze_news_llm(self, headline, description):
        text = f"Headline: {headline}\nDescription: {description}"

        prompt = f"""
        Analyze this news item.

        TEXT:
        {text}

        TASK:
        1. Extract MAIN entities (Companies, Funds, People, Locations).
        2. Determine if there is a direct interaction between two entities.

        Return JSON structure:
        {{
            "entities": ["Entity1", "Entity2"],
            "interaction": {{
                "source": "Entity1",
                "target": "Entity2",
                "relation": "PARTNERSHIP",
                "summary": "Max 10 words summary"
            }},
            "sentiment": "POSITIVE" / "NEGATIVE" / "NEUTRAL"
        }}

        Rules:
        - If no interaction, set "interaction": null.
        - JSON ONLY.
        """
        try:
            completion = self.client.chat.completions.create(
                model='qwen/qwen-2.5-7b-instruct',
                messages=[{'role': 'user', 'content': prompt}],
                temperature=0.0
            )
            content = completion.choices[0].message.content

            clean_content = content.replace("```json", "").replace("```", "").strip()

            parsed = json.loads(clean_content)
            if isinstance(parsed, list):
                return parsed[0] if len(parsed) > 0 else None

            return parsed

        except Exception as e:
            tqdm.write(f"üõë API ERROR: {str(e)}")
            return None


    def process_csv(self, file_path):
        print(f"\nüöÄ –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π: {file_path}")

        if not os.path.exists(file_path):
            print("‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return

        df = pd.read_csv(file_path)

        stats = {"edges": 0, "news_nodes": 0, "skipped": 0}

        with self.driver.session() as session:
            for i, row in tqdm(df.iterrows(), total=len(df), desc="Processing"):

                headline = str(row.get('headline', ''))
                desc = str(row.get('text', ''))
                date_str = str(row.get('Time', datetime.now()))

                tqdm.write(f"\nüìÑ News [{i + 1}]: {headline[:100]}...")

                analysis = self.analyze_news_llm(headline, desc)
                if not analysis:
                    tqdm.write("   ‚ö†Ô∏è LLM –≤–µ—Ä–Ω—É–ª–∞ –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç.")
                    stats['skipped'] += 1
                    continue

                raw_names = analysis.get('entities', [])
                if isinstance(raw_names, str): raw_names = [raw_names]

                tqdm.write(f"   üß† LLM –≤—ã–¥–µ–ª–∏–ª–∞: {raw_names}")

                found_entities = []
                seen_uids = set()

                for name in raw_names:
                    match = self.resolve_entity(name, threshold=0.62)
                    if match:
                        uid = f"{match['type']}_{match['id']}"
                        if uid not in seen_uids:
                            found_entities.append(match)
                            seen_uids.add(uid)
                            tqdm.write(
                                f"      ‚úÖ Match: '{name}' -> {match['name']} ({match['type']}) [Score: {match['score']:.2f}]")
                    else:
                        pass
                        # tqdm.write(f"      ‚ùå No match: '{name}'")

                if not found_entities:
                    tqdm.write("   ‚ö†Ô∏è –í –±–∞–∑–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–π —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ —Å–ø–∏—Å–∫–∞.")
                    stats['skipped'] += 1
                    continue

                try:
                    iso_date = datetime.strptime(str(date_str).strip(), "%b %d %Y").strftime("%Y-%m-%d")
                except:
                    iso_date = datetime.today().strftime("%Y-%m-%d")

                sentiment = analysis.get('sentiment', 'NEUTRAL')
                interaction = analysis.get('interaction')
                log_entry = f"[{iso_date}] {sentiment}: {headline}"

                link_created = False

                # –°–æ–∑–¥–∞–µ–º —Å–≤—è–∑—å
                if interaction and isinstance(interaction, dict) and len(found_entities) >= 2:
                    src_obj = self.resolve_entity(interaction.get('source'), 0.60)
                    trg_obj = self.resolve_entity(interaction.get('target'), 0.60)

                    if not src_obj: src_obj = found_entities[0]
                    if not trg_obj and len(found_entities) > 1: trg_obj = found_entities[1]

                    if src_obj and trg_obj and src_obj['id'] != trg_obj['id']:
                        rel_type = interaction.get('relation', 'RELATED_TO').upper().replace(" ", "_")
                        summary = interaction.get('summary', headline)

                        cypher_rel = "RELATED_TO"
                        if "PARTNER" in rel_type:
                            cypher_rel = "PARTNER_WITH"
                        elif "DISPUTE" in rel_type:
                            cypher_rel = "IN_DISPUTE_WITH"
                        elif "INVEST" in rel_type:
                            cypher_rel = "INVESTED_IN"
                        elif trg_obj['type'] in ['City', 'Country']:
                            cypher_rel = "AFFECTS_REGION"

                        tqdm.write(f"   üîó LINK: {src_obj['name']} -[{cypher_rel}]-> {trg_obj['name']}")

                        query_edge = f"""
                        MATCH (a:{src_obj['type']} {{ {src_obj['key_field']}: $id1 }})
                        MATCH (b:{trg_obj['type']} {{ {trg_obj['key_field']}: $id2 }})
                        MERGE (a)-[r:{cypher_rel}]->(b)
                        ON CREATE SET r.created_at = date($date), r.news_history = [$log], r.last_summary = $sum
                        ON MATCH SET r.news_history = r.news_history + $log, r.last_updated = date($date)
                        """
                        session.run(query_edge, id1=src_obj['id'], id2=trg_obj['id'],
                                    date=iso_date, log=log_entry, sum=summary)

                        stats['edges'] += 1
                        link_created = True

                # –í–µ—à–∞–µ–º –Ω–æ–≤–æ—Å—Ç—å –Ω–∞ —É–∑–ª—ã
                if not link_created:
                    names_list = [e['name'] for e in found_entities]
                    tqdm.write(f"   üìå ATTACH: –ù–æ–≤–æ—Å—Ç—å –ø—Ä–∏–∫—Ä–µ–ø–ª–µ–Ω–∞ –∫: {names_list}")

                    for ent in found_entities:
                        query_news = f"""
                        MATCH (e:{ent['type']} {{ {ent['key_field']}: $id }})
                        MERGE (n:News {{headline: $headline, date: date($date)}})
                        SET n.sentiment = $sent
                        MERGE (n)-[:MENTIONS]->(e)
                        """
                        session.run(query_news, id=ent['id'], headline=headline, date=iso_date, sent=sentiment)
                    stats['news_nodes'] += 1

        print(
            f"\nüèÅ –ò—Ç–æ–≥–∏: –°–≤—è–∑–µ–π —Å–æ–∑–¥–∞–Ω–æ: {stats['edges']}, –ù–æ–≤–æ—Å—Ç–µ–π-—É–∑–ª–æ–≤: {stats['news_nodes']}, –ü—Ä–æ–ø—É—â–µ–Ω–æ: {stats['skipped']}")

if __name__ == "__main__":
    rag = NewsGraphPipeline()

    rag.setup_knowledge_base_vectors()

    rag.process_csv('../data/classified_reuters_news_mapped.csv')

    rag.close()